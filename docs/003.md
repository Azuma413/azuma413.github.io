---
title: "最近のVLA研究"
date: "2025-10-24"
excerpt: "こちらの記事を読んで，思うところがあったので自分の意見を書く．"
---

https://arxiv.org/abs/2203.17270

自動運転にしてもロボットマニピュレーションにしても、VLAはVLMをそのまま使った方が良いよね、という面白くない流れが来ている。
Action HeadやDiscrete Special Tokenすら無くして、直接VLMに制御コマンドをテキストとして出力させた方が、性能が高くなるって話。
VLAアーキテクチャに問題があると言うよりは、VLAの学習過程で、明らかにVLMが破滅的忘却を起こしているのが問題なのだが、それがアーキテクチャ由来だと言われれば、まぁそうかも。

LoRA使えば良いじゃんってよく言うけど、結局LoRAを適用した際のモデルが崩壊していたら、意味ないんだよね。
例えばVLMの事後学習段階ではテキストとしてActionを出力させる。その後、VLM重みを凍結して、Action Head（DiffusionやFlow Matching）を学習する。
という2段階の学習を行えば、VLMの事前学習知識は保存される気がする。

最近の研究をみていると、VLAが回帰や拡散で行動を事後学習する際に、元のVLMバックボーンが破滅的忘却を引き起こしているという指摘があります。
これを踏まえて、現状、大きくわけて2つの方針があります。1つはVLM2VLAやVLA-0のように、行動をSpecial TokenやAction Headを用いた数値ではなく、テキストとして表現してしまう方法です。これにより、事後学習データと事前学習データの差が縮まり、VLMの知識が保存され、汎化性能が向上します。
2つ目は、EO-1やSimLingo、Pi0.5などのように、事後学習においても行動出力とテキスト出力を組み合わせた学習方法を取り入れ、VLMの崩壊を防ぐ方法です。
これらを踏まえて、個人的に考えている手法があります。まず、VLMはLoRAを使いつつ、テキストを行動として出力するように追加学習します。次に、VLMを凍結した状態で、中規模のTransformerに、VLMの潜在表現や画像特徴量などを条件付けた上で、DiffusionもしくはFlow Matching損失で連続行動を出力するよう学習する方法です。これのメリットは、VLMの汎化性能を維持しつつ、Diffusionなどによるマルチモーダルな連続行動を生成できる点です。また、Action Expertのみを独立して学習することも出来るため、触覚や聴覚など、他のモーダルの追加がやりやすい上に、強化学習の適用もしやすいと考えられます。

例えばVLAのAction ExpertをEBMで構成したら、VLA全体をWaddington地形とのアナロジーで捉えられる……かもしれない。 VLM出力など、Action ExpertにConditioningされている要素が、エネルギー地形を変化させ、そこを球が転がるようにして、行動がある程度ロバストに決定される。